# ðŸ”¹ 1. Â¿QuÃ© es Apache Spark?

Apache Spark es un framework de procesamiento distribuido de datos en clÃºster, diseÃ±ado para ser rÃ¡pido y tolerante a fallos.
Permite trabajar con Big Data en memoria, a diferencia de Hadoop MapReduce (que escribe/lee mucho en disco).

Usos principales:
- Procesamiento batch (ETL masivos).
- Procesamiento en streaming.
- Machine Learning con MLlib.
- AnÃ¡lisis SQL con SparkSQL.
- GrÃ¡ficos con GraphX.

Se puede programar en Scala, Java, Python (PySpark) y R.

# ðŸ”¹ 2. InstalaciÃ³n de Apache Spark
Dependiendo de tu entorno:

ðŸ–¥ï¸ InstalaciÃ³n local (modo standalone)

Requisitos previos

Tener instalado Java (JDK 8 o superior)
```bash
java -version
```
Tener instalado Python 3.x si quieres usar PySpark.

Instalar Scala (opcional, si vas a usar Spark en Scala).

Descargar Spark
Desde la pÃ¡gina oficial: https://spark.apache.org/downloads.html

Elige versiÃ³n estable (ej: Spark 3.5.x).

Selecciona pre-built for Hadoop (ej: Hadoop 3.3).

Extraer e instalar
```bash
tar -xvf spark-3.5.0-bin-hadoop3.tgz
mv spark-3.5.0-bin-hadoop3 /usr/local/spark
```

Configurar variables de entorno (ejemplo en Linux/Mac, en Windows se hace en Variables de Entorno):
```bash
export SPARK_HOME=/usr/local/spark
export PATH=$SPARK_HOME/bin:$PATH
```

Probar instalaciÃ³n
```bash
spark-shell   # para usar Scala
pyspark       # para usar Python
```
ðŸ InstalaciÃ³n con PySpark (solo Python)

Si no quieres descargar Spark manualmente, puedes instalar con pip:
```bash
pip install pyspark
```

Esto descarga Spark y Java embebido, suficiente para pruebas locales.

â˜ï¸ InstalaciÃ³n en clÃºster

Spark tambiÃ©n se puede desplegar en:
- YARN (Hadoop)
- Kubernetes
- Standalone cluster

Pero eso ya es nivel avanzado.

# ðŸ”¹ 3. Uso bÃ¡sico de Spark

Ejemplo con PySpark:
```bash
from pyspark.sql import SparkSession

# Crear sesiÃ³n de Spark
spark = SparkSession.builder \
    .appName("EjemploSpark") \
    .getOrCreate()

# Crear un DataFrame
data = [("Rodrigo", 36), ("Joseline", 30), ("Juan", 25)]
columns = ["Nombre", "Edad"]

df = spark.createDataFrame(data, columns)

# Operaciones
df.show()
df.filter(df["Edad"] > 30).show()

# Usar SQL
df.createOrReplaceTempView("personas")
spark.sql("SELECT * FROM personas WHERE Edad > 30").show()

spark.stop()
```

Salida esperada:

+--------+----+
|  Nombre|Edad|
+--------+----+
| Rodrigo|  36|
|Joseline|  30|
|    Juan|  25|
+--------+----+

+--------+----+
|  Nombre|Edad|
+--------+----+
| Rodrigo|  36|
+--------+----+

# ðŸ”¹ 4. Comandos Ãºtiles

spark-shell â†’ iniciar en Scala.

pyspark â†’ iniciar en Python.

spark-submit script.py â†’ ejecutar un script en Spark (local o clÃºster).